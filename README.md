 # 迁移学习、数据增强、微调的中文自学笔记
 
深度学习往往需要大量算力和数据，但在很多情况下经济实力不允许，大算力遥不可及；另外，当数据集比较小，而且没有办法获取更多数据或制作标注数据比较困难时，如何获取更高的正确率呢？迁移学习、数据增强和微调将给出破题之道。  

所谓迁移学习，就是使用在大规模数据集上训练好的模型来解决小数据集问题。这种在大规模数据集上训练好的模型一般称为预训练模型或预训练网络。思路是利用预训练模型的卷积部分（也称卷积基）提取数据集的特征，然后重新训练最后的全连接部分（也称分类器）。在这个特征提取过程中，要确保预训练模型的特征提取部分（也就是卷积基的参数）不能发生变化。迁移学习的思路有以下3步：  
 
(1)冻结预训练模型的卷积基；  
 
(2)根据具体问题重新设置分类器；  
 
(3)用自己的数据集训练设置好的分类器；  
 
torchvision库的models模块为我们提供了常见的预训练模型，这些预训练模型是在ImageNet数据集的子集（140万个标记图像，1000个不同的类别）上训练好的大型卷积神经网络。常见的预训练模型包括ResNet、VGG、Inception、DenseNet等。
